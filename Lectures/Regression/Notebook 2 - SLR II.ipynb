{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLR II\n",
    "\n",
    "In SLR I we learned about the statistical learning approach to problems, and then saw it in action with the Simple Linear Regression algorithm. In this notebook you'll dive deeper into simple linear regression, this will prepare us to build upon SLR with multiple and polynomial linear regression in the next two notebooks.\n",
    "\n",
    "## What We'll Accomplish in This Notebook\n",
    "\n",
    "Specifically we'll cover:\n",
    "<ul>\n",
    "    <li>An aside on the difference between explanatory modeling and predictive modeling.</li> \n",
    "    <li>SLR assumptions and how to check them including:</li>\n",
    "        <ul>\n",
    "            <li>The relationship between $X$ and $y$ is linear,</li> \n",
    "            <li>The variance of the errors, $\\epsilon_i$, are the same for all $i$,</li> \n",
    "            <li>The errors are normally distributed,</li> \n",
    "            <li>Each observation is independent of all other observations.</li> \n",
    "        </ul>\n",
    "    <li>Pearson's correlation coefficient.</li> \n",
    "    <li>Touching on $p$-values and hypothesis testing for the coefficient, $\\beta_1$.</li> \n",
    "    <li>Touching on confidence intervals for $\\beta_1$ and our line.</li> \n",
    "</ul>\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we'll use\n",
    "\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Aside - Explanatory vs. Predictive Modeling\n",
    "\n",
    "Now many of you have probably taken a course or two in statistics, and at least some of you have probably learned simple linear regression in that setting.\n",
    "\n",
    "But the whole concept of train test split we introduced in SLR I probably seemed a bit foreign to anyone that's seen SLR before. \n",
    "\n",
    "#### What's the Reason for the Difference?\n",
    "\n",
    "In a traditional stats class you're learning about explanatory statistical models, whereas in this course we're primarily focusing on predictive modeling. These two fields can use the same tools, but have somewhat different goals.\n",
    "\n",
    "##### Explanatory Modeling\n",
    "\n",
    "The goal here is to build models that explain the data we've observed the best that we can. This is really important in scientific research where we're trying to provide explanations as to why the world is the way it is. In explanatory modeling you'll often see things like hypothesis tests, $p$-values, and confidence intervals when we examine how \"good\" a model is, or how <i>statistically significant</i> the results are.\n",
    "\n",
    "##### Predictive Modeling\n",
    "\n",
    "The goal here is to build models that predict really good. As we mentioned before in SLR I you can build a model that explains the data really well, but that model is not then guaranteed to also have good predictive power. We haven't touched on how we can improve our model's predictive power, but in the multiple linear regression notebook we'll introduce the technique of <i>cross validation</i>. In many predictive modeling settings you're willing to accept a model that might not best explain the data, as long as it provides superior predictions.\n",
    "\n",
    "#### Frenemies?\n",
    "\n",
    "As you can see these two approaches don't always align, and as with any binary choice there are people on both sides of the aisle that passionately disagree with the other side. However, the two really coexist and which approach is best depends upon the problem at hand. If the comparison between the two is interesting to you, check out this paper <a href=\"https://www.stat.berkeley.edu/~aldous/157/Papers/shmueli.pdf\">https://www.stat.berkeley.edu/~aldous/157/Papers/shmueli.pdf</a> by Galit Shmueli who was at the University of Maryland at the time.\n",
    "\n",
    "### Moving Forward\n",
    "\n",
    "One of the great things about being a data scientist (in my opinion) is that you get the opportunity to do both, but that means you have to know both (not always! it is becoming more and more common that people are hired to do specific roles). For about $90\\%$ of this course we'll approach things from a predictive modeling point of view. However, because linear regression is such a fundamental technique we will cover it from the explanatory modeling point of view as well.\n",
    "\n",
    "## Data Science Aside Over\n",
    "\n",
    "## Break Time!\n",
    "\n",
    "Let's take a 1 to 2 minute break for questions.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Break Time Over!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will look at things from an explanatory modeling approach, to do this we'll examine how the model explains the training data from SLR I. Note, this is why we put in random seeds!\n",
    "\n",
    "## SLR Modeling Assumptions\n",
    "\n",
    "With any statistical model you make a number of assumptions, this is what allows you to develop some algorithm to estimate the model. We'll now go over the three key assumptions in SLR, then show you how you can check them. We'll use our baseball data to demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "baseball = pd.read_csv(\"baseball_run_diff.csv\")\n",
    "\n",
    "# Train test split\n",
    "baseball_copy = baseball.copy()\n",
    "baseball_train = baseball_copy.sample(frac = .75, random_state = 440)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption 1 - A Linear Relationship\n",
    "\n",
    "There exists a linear relationship between $y$ and $X$. This one is pretty straightforward. If we are going to model $y$ as a linear function of $X$ then we better think that one actually exists.\n",
    "\n",
    "#### How to Check\n",
    "\n",
    "##### Make a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use plt.scatter for this\n",
    "\n",
    "# first make a figure\n",
    "# this makes a figure that is 10 units by 10 units\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "# plt.scatter plots RD on the x and W on the y\n",
    "plt.scatter(baseball_train.RD, baseball_train.W)\n",
    "\n",
    "# Always good practice to label well when\n",
    "# presenting a figure to others\n",
    "# place an xlabel\n",
    "plt.xlabel(\"Run Differential\", fontsize =16)\n",
    "\n",
    "# place a ylabel\n",
    "plt.ylabel(\"Wins\", fontsize = 16)\n",
    "\n",
    "# type this to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or\n",
    "\n",
    "##### Look at the Pearson Correlation, $\\rho$\n",
    "\n",
    "This is a statistical measure of the <i>strength of the linear relationship</i> between $y$ and $X$. Here's the formula:\n",
    "$$\n",
    "\\rho = \\frac{Cov(y,X)}{\\sigma_X \\sigma_y}\n",
    "$$\n",
    "\n",
    "You can approximate $\\rho$ with the sample covariance and sample standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate it here\n",
    "\n",
    "# pandas has a nice corr() function\n",
    "baseball_train[['W','RD']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the Pearson Correlation is bounded between $-1$ and $1$ with more positive values indicating a strong positive linear relationship, and more negative values indicating a strong negative linear relationship. Our correlation of $0.933$ here indicates that wins and run differential have a strong positive linear relationship.\n",
    "\n",
    "For those looking for more statistical rigor, check out this page, <a href=\"https://online.stat.psu.edu/stat501/lesson/1/1.9\">https://online.stat.psu.edu/stat501/lesson/1/1.9</a> to see how you can actually perform a hypothesis test on whether or not there is statistically significant evidence that $\\rho \\neq 0$.\n",
    "\n",
    "Note that getting a $\\rho$ close to $0$ does NOT mean there is no relationship, just no linear relationship. For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an x and y\n",
    "x = np.linspace(-2,2,1000)\n",
    "\n",
    "# y = x^2\n",
    "y = np.power(x,2)\n",
    "\n",
    "# what is the correlation rounded to 10 decimal places?\n",
    "print(\"The Pearson correlation between x and y is\",np.round(np.corrcoef(x,y)[0,1],10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Break!\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Break Over!\n",
    "\n",
    "### Assumption 2 - All Observations are Independent\n",
    "\n",
    "We also assume that each observation in the data set is independent from all other observations.\n",
    "\n",
    "#### How to Check\n",
    "\n",
    "This is slightly harder to check then assessing a linear fit. But there are some ways to test.\n",
    "\n",
    "##### Thinking About How the Data Was Collected\n",
    "\n",
    "This approach helps if you know something about the data collection process. For example, if you wanted to know something about OSU undergrads and you randomly sampled people from a list in the registrars office that would produce independent observations. But if you randomly sampled people from a single Calculus Class and a single Art History Class then your observations are likely dependent.\n",
    "\n",
    "In our baseball example, we do have reason for concern. The data is produced from year after year observations of the same teams. We did sample our data randomly, but its possible that there is time dependence or team depence. Which takes us to the next way to check.\n",
    "\n",
    "##### Making more plots\n",
    "\n",
    "Plot your residuals (this is what we call the difference between the predicted values and the actual values, $w_i - \\hat{w_i}$) against your feature and other variables of concern. Then you examine the plots to see if there is an obvious relationship. Let's do that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for prediction\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the model object\n",
    "reg = LinearRegression(copy_X = True)\n",
    "\n",
    "# fit the model\n",
    "reg.fit(baseball_train.RD.values.reshape(-1,1),baseball_train.W.values.ravel())\n",
    "\n",
    "# predict\n",
    "pred = reg.predict(baseball_train.RD.values.reshape(-1,1))\n",
    "\n",
    "# Now get the residuals\n",
    "res = baseball_train.W.values.ravel() - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First plot of rd vs residual\n",
    "\n",
    "plt.figure(figsize = (12,12))\n",
    "\n",
    "plt.scatter(baseball_train.RD,res)\n",
    "\n",
    "plt.xlabel(\"Run Differential\", fontsize = 16)\n",
    "plt.ylabel(\"Residual\", fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot of run differential vs residual it seems that the two have no obvious relationship, which is a good sign for our inderpendence assumption. Let's move on to residual vs year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot year vs residual\n",
    "\n",
    "plt.figure(figsize = (12,12))\n",
    "\n",
    "plt.scatter(baseball_train.yearID,res)\n",
    "\n",
    "plt.xlabel(\"Year\", fontsize = 16)\n",
    "plt.ylabel(\"Residual\", fontsize = 16)\n",
    "\n",
    "# Note this allows you to set specific tick mark values\n",
    "plt.xticks(range(2000,2020))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another blob with no obvious pattern, excellent! This is more good news for us in terms of the independence assumption. Let's now look at residuals vs team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot team vs residual\n",
    "\n",
    "plt.figure(figsize = (16,12))\n",
    "\n",
    "plt.scatter(baseball_train.teamID,res)\n",
    "\n",
    "plt.xlabel(\"Team\", fontsize = 16)\n",
    "plt.ylabel(\"Residual\", fontsize = 16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is a little worrying. Some teams seem to have their wins consistently overestimated (a negative residual) like ARIzona, while others are more likely to be underestimated (a positive residual) like MINnesota. So there is reason to believe that residuals are not independent over team. There are ways to handle this, but they fall out of the scope of this notebook. For this data our violation doesn't seem too egregious so we'll soldier on. \n",
    "\n",
    "### Assumption 3 - The Residuals are Normally Distributed with Mean 0 and Equal Variance\n",
    "\n",
    "The final assumption is on the distribution of the residuals. We say that $\\epsilon_i \\sim N(0,\\sigma^2)$ for all $i$. The assumption on variance is called the homoscedasticity assumption.\n",
    "\n",
    "#### How to Check Normality\n",
    "\n",
    "##### Make More Plots!\n",
    "\n",
    "We can make a histogram of the residuals to check for a bell curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "\n",
    "plt.hist(res)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That certainly looks centered around zero and like a bell curve! But histograms are tough because there are non-normal distributions that look close to the bell curve, for example the Cauchy distribution, <a href=\"https://en.wikipedia.org/wiki/Cauchy_distribution\">https://en.wikipedia.org/wiki/Cauchy_distribution</a>.\n",
    "\n",
    "So the standard plot you'll see is a quantile-quantile plot, or Q-Q plot. This matches the quantiles of the data with the quantiles of a normal distribution and plots them against eachother. If the residuals are normally distributed, these should fall on a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This package allows us to make a q-q plot\n",
    "import statsmodels.api as sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "# qqplot makes the qqplot\n",
    "# put in the data you want to plot\n",
    "# line = 's' plots a line fit to our data\n",
    "# ax=ax allows us to put the data on the plt subplot object I made\n",
    "sm.qqplot(res,line='s',ax=ax) \n",
    "\n",
    "# Normals go on the x-axis\n",
    "plt.xlabel(\"Normal Quantiles\", fontsize=16)\n",
    "\n",
    "# Residuals on the y-axis\n",
    "plt.ylabel(\"Residual Quantiles\", fontsize=16)\n",
    "\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good! There's typically some slight bowing of the dots near the tails of the normal distribution. We should feel okay with our assumption that the baseball residuals are normally distributed.\n",
    "\n",
    "#### How to Check Homoscedasticity?\n",
    "\n",
    "##### Even More Plots!\n",
    "\n",
    "Now we plot the predicted values vs the residuals. If the residuals have equal variance you should expect to see most of the points fall in a band around $0$. We DON'T want to see the points opening up into a funnel shape or closing into a funnel shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "plt.scatter(pred,res)\n",
    "\n",
    "plt.xlabel(\"Predicted Values\", fontsize = 16)\n",
    "plt.ylabel(\"Residuals\", fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again this looks good to me! The residuals mostly fall between $10$ and $-10$ and I don't see anything that could be construed as a funnel! Looks like we have homoscedasticity.\n",
    "\n",
    "## Practice\n",
    "\n",
    "Return to the model you made for the `carseats` data. Go through and check all of the linear regression assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the Statistical Significance of the Model\n",
    "\n",
    "So far we've fit a model, interpreted the fit, and examined whether or not our modeling assumptions fit.\n",
    "\n",
    "For the baseball data we're fairly confident that our assumptions are okay. So the next thing we can do from an explanatory modeling point of view is to assess the model fit. In SLR I we took that as looking at the loss function, the MSE. But we can also look at other measures of model goodness.\n",
    "\n",
    "### $p$-value For $\\beta_1$\n",
    "\n",
    "One such goodness measure it to conduct a hypothesis test of whether or not there actually is a linear relationship between $y$ and $X$. In the view of SLR this means performing the following hypothesis test:\n",
    "$$\n",
    "\\text{H}_0: \\beta_1 = 0 \\text{ vs. }\n",
    "$$\n",
    "$$\n",
    "\\text{H}_1: \\beta_1 \\neq 0\n",
    "$$\n",
    "One way we can perform this test is to use the `statsmodel` package again.\n",
    "\n",
    "We'll fit the regression in `statsmodel` then examine the model's summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the data as np.arrays\n",
    "# reshape(-1,1) makes it a column vector\n",
    "y = np.array(baseball_train.W.values.reshape(-1,1))\n",
    "rd = np.array(baseball_train.RD.values.reshape(-1,1))\n",
    "\n",
    "# We need to add a column of ones in order to allow for\n",
    "# a constant\n",
    "X = np.concatenate([np.ones(np.shape(rd)),rd],axis=1)\n",
    "\n",
    "\n",
    "# Fit a simple linear regression model\n",
    "# sm.OLS stands for Ordinary Least Squares, this\n",
    "# is the name for the method used to get the coefficients\n",
    "# First put y, then X\n",
    "slr = sm.OLS(y, X)\n",
    "\n",
    "# Now fit the model\n",
    "fit = slr.fit()\n",
    "\n",
    "# fit.summary makes a snazy table for us to look at\n",
    "print(fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine the table.\n",
    "<img src=\"SLR_table_p_value.png\" style=\"width:80%;\"></img>\n",
    "The row with `x1` corresponds to what we called $\\beta_1$. We can see in the `coef` column the same estimate we calculated in SLR I, $0.0999$. If you move over to the `P>|t|` column, this is the $p$-value for the hypothesis test we are interested in, for this particular data our $p$-value is smaller than $0.0005$ which means that the probability of observing what we observed under the null hypothesis is very very small. This means we would reject the null hypothesis in favor of H$_1$. In non-statistical lingo, we have good reason to believe that there is a linear relationship between run differential and wins.\n",
    "\n",
    "## Question Break\n",
    "\n",
    "Okay let's take a quick question break for any pressing questions.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Break Over\n",
    "\n",
    "### Confidence Interval for $\\beta_1$\n",
    "\n",
    "We can also construct a $95\\%$ confidence interval for $\\beta_1$. Recall from our Probability Theory and Statistics Cheat Sheet that this can be found by taking $\\hat{\\beta_1} \\pm p_{\\hat{\\beta_1},.95} se(\\beta_1)$, under the assumptions of SLR the probability multiplier, $p_{\\hat{\\beta_1},.95}$ follows a studentized $t$ distribution with $n - 2$ degrees of freedom.\n",
    "\n",
    "We can go through the trouble of calculating this by hand, or we can rely on our prior `statsmodel` table\n",
    "<img src=\"SLR_table_CI.png\" style=\"width:80%;\"></img>\n",
    "So for the $\\beta_1$ from the baseball data our $95\\%$ confidence interval is $(0.096,0.104)$.\n",
    "\n",
    "\n",
    "## Question Break\n",
    "\n",
    "Let's take a quick one minute question break, before going to a breakout session.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Break Over\n",
    "\n",
    "### Practice\n",
    "\n",
    "Go through and perform the hypothesis test for $\\beta_1$ from the `carseats` data. Also construct a $95\\%$ confidence interval for $\\beta_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Interval for the Regression Line\n",
    "\n",
    "We'll end this notebook with a discussion on how we can provide some understanding on the uncertainty surrounding our regression line.\n",
    "\n",
    "<b>Warning:</b> We will be delving into a little bit of probability theory here! \n",
    "\n",
    "Let's take a look back at the model we're fitting:\n",
    "$$\n",
    "y = f(X) + \\epsilon = \\beta_0 + \\beta_1X + \\epsilon,\n",
    "$$\n",
    "where $\\epsilon$ is a vector of independent $\\epsilon_i \\sim N(0,\\sigma^2)$ for all $i$.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Now take the expectation on both sides:\n",
    "$$\n",
    "E(y) = E(\\beta_0 + \\beta_1X + \\epsilon) = \\beta_0 + \\beta_1E(X) + E(\\epsilon) = \\beta_0 + \\beta_1E(X),\n",
    "$$\n",
    "this is where we got the formula for $\\hat{\\beta_0}$. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Now let's say we know the value of $X$ for example $X^*$, in probability terms we're now looking at $y$ conditional on $X=X^*$ (denoted $y|X=X^*$), and then take the expectation:\n",
    "$$\n",
    "E(y|X=X^*) =  \\beta_0 + \\beta_1E(X^*) = \\beta_0 + \\beta_1 X^*,\n",
    "$$\n",
    "because we are looking at a specific value of $X$ so it is no longer random. \n",
    "\n",
    "<br>\n",
    "\n",
    "So the regression line we've been plotting is actually a series of point estimates for the mean value of $y$ given a specific value of $X$. We've been denoting these point estimates as $\\hat{y}$. \n",
    "\n",
    "Just like we gave a confidence interval for $\\beta_1$ using our point estimate $\\hat{\\beta_1}$ we can give a confidence interval for $y|X$ using our point estimate $\\hat{y}$. The formula for the confidence interval for $E(y|X=X^*)$ is:\n",
    "$$\n",
    "\\hat{y} \\pm t_{n-2,(1-\\alpha/2)}\\sqrt{\\frac{\\sum_{i=1}^n\\left(y_i - \\hat{y_i}\\right)^2}{n-2}}\\sqrt{\\frac{1}{n} + \\frac{\\left(X^* - \\overline{X}\\right)^2}{(n-1)s_X^2}},\n",
    "$$\n",
    "where $n$ is the number of observations and $t_{n-2,(1-\\alpha/2)}$ is such that $P(T\\geq t_{n-2,(1-\\alpha/2)}) = \\alpha/2$ for a random variable $T$ with a Studentized $t$ distribution with $n-2$ degrees of freedom. This formula still follows the confidence interval pattern, where here the product of the square roots is the standard error of $E(y|X=X^*)$.\n",
    "\n",
    "## Questions\n",
    "\n",
    "Let's take one or two questions then we'll write some code to get a confidence interval.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Question Time Over\n",
    "\n",
    "Let's see how we can implement this with code!\n",
    "\n",
    "Unfortunately there is no easy built in confidence interval code, so we'll make a function by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run this chunk to make sure we have \n",
    "# the model and residuals from earlier\n",
    "# make the model object\n",
    "reg = LinearRegression(copy_X = True)\n",
    "\n",
    "# fit the model\n",
    "reg.fit(baseball_train.RD.values.reshape(-1,1),baseball_train.W.values.ravel())\n",
    "\n",
    "# predict\n",
    "pred = reg.predict(baseball_train.RD.values.reshape(-1,1))\n",
    "\n",
    "# Now get the residuals\n",
    "res = baseball_train.W.values.ravel() - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scipy package allows you to get the t stat we need\n",
    "from scipy.stats import t\n",
    "\n",
    "# We'll write a function that takes in X, X_star, alpha,\n",
    "# and the residuals and produces a confidence interval around yhat\n",
    "def get_ci(X, X_star, res, alpha):\n",
    "    # Get n\n",
    "    n = len(res)\n",
    "\n",
    "    # The first square root we'll denote root_1\n",
    "    # remember our residuals were stored in res\n",
    "    root_1 = np.sqrt(np.sum(np.power(res,2))/(n-2))\n",
    "    \n",
    "    # This is the second square root, we'll call it root_2\n",
    "    root_2 = np.sqrt((1/n) + np.power((X_star-np.mean(X)),2)/((n-1)*np.var(X)))\n",
    "    \n",
    "    # get t_stat\n",
    "    # this is done using t.ppf 1-alpha goes first then the df\n",
    "    t_stat = t.ppf(1-alpha/2, n-2)\n",
    "    \n",
    "    # Now we calculate y_hat\n",
    "    y_hat = reg.predict(np.array(X_star).reshape(-1,1))\n",
    "    \n",
    "    # Now put it all together\n",
    "    lb = y_hat - t_stat*root_1*root_2\n",
    "    ub = y_hat + t_stat*root_1*root_2\n",
    "    \n",
    "    return lb,ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we can look at the ci for one value of X*\n",
    "# Let's say a team with 40 rd\n",
    "lb,ub = get_ci(baseball_train.RD.values,40,res,0.05)\n",
    "\n",
    "print(\"A 95% confidence interval for the average wins of a team with 40 rd is\",\n",
    "         lb[0],ub[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can even plot the confidence bounds around our regression line\n",
    "# Let's try a 99% confidence interval this time\n",
    "\n",
    "# Make the figure\n",
    "plt.figure(figsize = (12,12))\n",
    "\n",
    "# We want a wide array of rd values\n",
    "xs = np.linspace(-350,350,700)\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(xs, reg.predict(xs.reshape(-1,1)), 'b', label=\"Regression Line\")\n",
    "\n",
    "# Now we get the confidence bounds\n",
    "lbs,ubs = get_ci(baseball_train.RD.values,xs,res,0.01)\n",
    "\n",
    "# Plot the bounds\n",
    "plt.plot(xs, lbs, 'r--', label=\"99% Confidence Bounds\")\n",
    "plt.plot(xs, ubs, 'r--')\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel(\"Run Differential\", fontsize=16)\n",
    "plt.ylabel(\"Wins\", fontsize=16)\n",
    "\n",
    "# add a legend\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. Now I'll answer one or two questions, then it's your turn!\n",
    "\n",
    "## Questions\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## End Questions\n",
    "\n",
    "### Practice\n",
    "\n",
    "Make a function that takes in a confidence level, $(1-\\alpha)$, and then produces confidence bounds on your `carseats` data regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to confidence intervals you can produce what are known as prediction intervals which give you an idea of the uncertainty involved in predicting a value of $y$ given $X = X^*$. See the homework if you want to learn more.\n",
    "\n",
    "## One Final Note\n",
    "\n",
    "Let's look at our baseball regression line again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data = baseball_train, x = \"RD\", y = \"W\", height = 12)\n",
    "\n",
    "plt.xlabel(\"Run Differential\", fontsize=16)\n",
    "plt.ylabel(\"Wins\", fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look, our data doesn't go much further out than a $200$ run differential.\n",
    "\n",
    "So would it make sense to use our regression to predict the wins for a $300$ run differential or say $400$ run differential?\n",
    "\n",
    "If we did we would be flying blind. We can only safely predict using the data we have. While the relationship between wins and run differential appears linear from our data set, we don't know if that trend will continue beyond about $250$ run differential. \n",
    "\n",
    "This statement holds for more techniques than just regression. If someone tries to strong arm you into making predictions that fall far outside the limits of your training data be cautious because you don't know anything out that far.\n",
    "\n",
    "## See you in Notebook 3!\n",
    "\n",
    "That's it for notebook 2 and for simple linear regression. In the next notebook we'll get to look at models with more than one feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
